env_vars:
  MATHPIX_APP_ID: ''
  MATHPIX_APP_KEY: ''
  UNPAYWALL_EMAIL: ''
  SEMANTIC_SCHOLAR_API_KEY: ''
huggingface:
  use_auth: false
  local_model_path: ''
paths:
  project_root: .
  data_dir: data
  output_dir: output
  pdf_input_dir: ${paths.data_dir}/fetched_papers
  user_ids_csv: ${paths.data_dir}/user_ids.csv
  eval_benchmark_file: ${paths.data_dir}/benchmark_systems.json
  kb_output_dir: .
  kb_vector_store_filename: paper_index_mathpix.faiss
  kb_metadata_filename: paper_metadata_mathpix.jsonl
  ft_manual_data_file: ${paths.data_dir}/ft_manual_data.jsonl
  ft_extracted_data_file: ${paths.data_dir}/ft_extracted_data_verified.jsonl
  ft_combined_data_file: ${paths.data_dir}/ft_data_combined.jsonl
  ft_output_dir: ${paths.output_dir}/finetuning_results
  eval_results_file: ${paths.output_dir}/evaluation_results.csv
data_fetching:
  sleep_time_scholarly: 3
  sleep_time_api: 1.5
  sleep_time_retry: 5
  max_retries: 2
  requests_user_agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
    (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36
  publication_limit_per_author: 50
knowledge_base:
  embedding_model_name: all-mpnet-base-v2
  chunk_target_size_mmd: 1000
  chunk_overlap_mmd: 150
  mathpix_poll_max_wait_sec: 600
  mathpix_poll_interval: 10
  pipeline: "open_source"  # Pipeline for PDF processing: 'mathpix' or 'open_source'
  gpu_memory_limit: 4096   # Max VRAM to use in MB (set to ~4GB for RTX 3080)
  embedding_batch_size: 32  # Batch size for embedding generation
  low_memory_mode: false    # Enables memory optimizations (CPU fallback when necessary)

# Added embeddings section to match the structure expected by the code
embeddings:
  model_name: all-mpnet-base-v2
  chunk_size: 1000
  chunk_overlap: 150
  batch_size: 32           # Control batch size for embedding generation

fine_tuning:
  base_model_name: Qwen/Qwen2.5-15B-Instruct
  data_format: instruction
  use_adapter: true
  lora:
    r: 16
    alpha: 16
    dropout: 0.1
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
  quantization:
    use_4bit: true
    bnb_4bit_compute_dtype: float16
    bnb_4bit_quant_type: nf4
    use_nested_quant: false
  training:
    num_train_epochs: 1
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 4
    gradient_checkpointing: true
    max_grad_norm: 0.3
    learning_rate: 0.0002
    weight_decay: 0.001
    optim: paged_adamw_32bit
    lr_scheduler_type: cosine
    max_steps: -1
    warmup_ratio: 0.03
    group_by_length: true
    save_steps: 25
    logging_steps: 25
    packing: false
    max_seq_length: null
inference:
  rag_k: 3
  max_new_tokens: 512
  temperature: 0.6
  top_p: 0.9
evaluation:
  rag_k: ${inference.rag_k}
  max_new_tokens: ${inference.max_new_tokens}
  temperature: ${inference.temperature}
  top_p: ${inference.top_p}
  verification:
    num_samples_lie: 10000
    num_samples_boundary: 5000
    numerical_tolerance: 1.0e-06
    sos_default_degree: 2
    sos_epsilon: 1.0e-07
    optimization_max_iter: 100
    optimization_pop_size: 15
    attempt_sos: true
    attempt_optimization: true
