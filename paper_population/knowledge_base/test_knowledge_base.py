import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import logging
import argparse
import sys
from paper_population.utils.config_loader import load_config, DEFAULT_CONFIG_PATH # Import config loader

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Directories and filenames (point to Mathpix-based KB)
BASE_DIR = os.path.dirname(__file__)
DEFAULT_KB_DIR = os.path.join(BASE_DIR, "knowledge_base_mathpix") # <--- UPDATED
DEFAULT_VECTOR_STORE_FILENAME = "paper_index_mathpix.faiss"      # <--- UPDATED
DEFAULT_METADATA_FILENAME = "paper_metadata_mathpix.jsonl"    # <--- UPDATED (now .jsonl)

# Embedding Model (must match the one used for building)
EMBEDDING_MODEL_NAME = 'all-mpnet-base-v2'

# --- Functions ---

def load_knowledge_base(kb_dir, index_filename, metadata_filename):
    """Loads the FAISS index and metadata map from JSONL file."""
    index_path = os.path.join(kb_dir, index_filename)
    metadata_path = os.path.join(kb_dir, metadata_filename)

    if not os.path.exists(index_path) or not os.path.exists(metadata_path):
        logging.error(f"Knowledge base files not found in {kb_dir}.")
        logging.error(f"Please run the knowledge_base_builder.py script first.")
        return None, None

    try:
        logging.info(f"Loading FAISS index from {index_path}...")
        index = faiss.read_index(index_path)
        logging.info(f"Index loaded with {index.ntotal} vectors.")
    except Exception as e:
        logging.error(f"Failed to load FAISS index: {e}")
        return None, None

    try:
        logging.info(f"Loading metadata from {metadata_path} (JSONL format)...")
        metadata_map = {}
        with open(metadata_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    # The key is the chunk_id generated by the builder
                    chunk_id = data.get('chunk_id')
                    if chunk_id is not None:
                         # Store the whole object (text + metadata sub-object)
                         metadata_map[chunk_id] = data 
                    else:
                         logging.warning(f"Skipping line without 'chunk_id': {line.strip()}")
        logging.info(f"Metadata loaded for {len(metadata_map)} chunks.")
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding metadata JSONL line: {e} - in file {metadata_path}")
        return None, None
    except Exception as e:
        logging.error(f"Failed to load metadata JSONL from {metadata_path}: {e}")
        return None, None

    if index.ntotal != len(metadata_map):
        logging.warning(f"Mismatch between index size ({index.ntotal}) and metadata size ({len(metadata_map)}). Ensure metadata file is correct.")

    return index, metadata_map

def search_kb(query, model, index, metadata_map, k=5):
    """Embeds the query and searches the FAISS index."""
    if index is None or metadata_map is None:
        logging.error("Knowledge base not loaded. Cannot search.")
        return []

    logging.info(f"Embedding query: '{query[:100]}...'")
    try:
        query_embedding = model.encode([query])
        query_embedding = np.array(query_embedding).astype('float32')
    except Exception as e:
        logging.error(f"Failed to embed query: {e}")
        return []

    logging.info(f"Searching index for top {k} results...")
    try:
        distances, indices = index.search(query_embedding, k)
        results = []
        if len(indices) > 0:
            for i, idx in enumerate(indices[0]):
                if idx != -1:
                    # metadata_map now holds the full object {chunk_id, text, metadata}
                    # The key `idx` directly corresponds to `chunk_id`
                    chunk_data = metadata_map.get(idx)
                    if chunk_data:
                        results.append({
                            'index_id': idx,
                            'distance': float(distances[0][i]),
                            'chunk_data': chunk_data # Pass the whole retrieved object
                        })
                    else:
                        logging.warning(f"Metadata not found for index {idx}.")
        logging.info(f"Found {len(results)} relevant chunks.")
        return results
    except Exception as e:
        logging.error(f"Error during FAISS search: {e}")
        return []

# --- Main Execution ---

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test the paper knowledge base.")
    parser.add_argument("query", type=str, help="The search query to run against the knowledge base.")
    parser.add_argument("-k", type=int, default=None, help="Number of results to retrieve (default: from config).")
    parser.add_argument("--config", type=str, default=DEFAULT_CONFIG_PATH,
                        help="Path to the configuration YAML file.")
    args = parser.parse_args()

    # Load configuration using the provided or default path
    cfg = load_config(args.config)

    # Determine number of results 'k'
    k_results = args.k if args.k is not None else cfg.inference.rag_k # Use CLI arg or default from inference config

    # Get KB info from config
    kb_directory_to_use = cfg.paths.kb_output_dir
    index_filename = cfg.paths.kb_vector_store_filename
    metadata_filename = cfg.paths.kb_metadata_filename
    embedding_model_name = cfg.knowledge_base.embedding_model_name

    # 1. Load Embedding Model
    logging.info(f"Loading embedding model: {embedding_model_name}...")
    try:
        model = SentenceTransformer(embedding_model_name)
    except Exception as e:
        logging.error(f"Failed to load embedding model '{embedding_model_name}': {e}")
        sys.exit(1)

    # 2. Load Knowledge Base
    index, metadata_map = load_knowledge_base(kb_directory_to_use, index_filename, metadata_filename)
    if index is None or metadata_map is None:
        sys.exit(1)

    # 3. Search (Use k_results)
    search_results = search_kb(args.query, model, index, metadata_map, k=k_results)

    # 4. Print Results (Adjusted for new metadata structure)
    print(f"\n--- Search Results for: '{args.query}' (Top {k_results}) ---")
    if not search_results:
        print("No relevant chunks found.")
    else:
        for i, result in enumerate(search_results):
            chunk_info = result['chunk_data'] # Contains text and metadata sub-dict
            meta = chunk_info.get('metadata', {}) # Safely get metadata sub-dict
            print(f"\nResult {i+1} (Distance: {result['distance']:.4f}):")
            print(f"  Source: {meta.get('source', 'N/A')}")
            print(f"  Title (heuristic): {meta.get('potential_title', 'N/A')}")
            # Pages might be 'unknown' now with Mathpix
            print(f"  Pages: {meta.get('pages', 'N/A')}") 
            print(f"  Paragraph Index Range: {meta.get('start_para_index', '?')} - {meta.get('end_para_index', '?')}")
            print(f"--- Chunk Text (Index: {result['index_id']}) --- ")
            print(chunk_info.get('text', '[Error retrieving text]')) # Get text from chunk_info
            print("---------------------------------")

    logging.info("Test script finished.") 