import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import logging
import argparse
import sys

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Directories and filenames (point to Mathpix-based KB)
BASE_DIR = os.path.dirname(__file__)
DEFAULT_KB_DIR = os.path.join(BASE_DIR, "knowledge_base_mathpix") # <--- UPDATED
DEFAULT_VECTOR_STORE_FILENAME = "paper_index_mathpix.faiss"      # <--- UPDATED
DEFAULT_METADATA_FILENAME = "paper_metadata_mathpix.jsonl"    # <--- UPDATED (now .jsonl)

# Embedding Model (must match the one used for building)
EMBEDDING_MODEL_NAME = 'all-mpnet-base-v2'

# --- Functions ---

def load_knowledge_base(kb_dir, index_filename, metadata_filename):
    """Loads the FAISS index and metadata map from JSONL file."""
    index_path = os.path.join(kb_dir, index_filename)
    metadata_path = os.path.join(kb_dir, metadata_filename)

    if not os.path.exists(index_path) or not os.path.exists(metadata_path):
        logging.error(f"Knowledge base files not found in {kb_dir}.")
        logging.error(f"Please run the knowledge_base_builder.py script first.")
        return None, None

    try:
        logging.info(f"Loading FAISS index from {index_path}...")
        index = faiss.read_index(index_path)
        logging.info(f"Index loaded with {index.ntotal} vectors.")
    except Exception as e:
        logging.error(f"Failed to load FAISS index: {e}")
        return None, None

    try:
        logging.info(f"Loading metadata from {metadata_path} (JSONL format)...")
        metadata_map = {}
        with open(metadata_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    # The key is the chunk_id generated by the builder
                    chunk_id = data.get('chunk_id')
                    if chunk_id is not None:
                         # Store the whole object (text + metadata sub-object)
                         metadata_map[chunk_id] = data 
                    else:
                         logging.warning(f"Skipping line without 'chunk_id': {line.strip()}")
        logging.info(f"Metadata loaded for {len(metadata_map)} chunks.")
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding metadata JSONL line: {e} - in file {metadata_path}")
        return None, None
    except Exception as e:
        logging.error(f"Failed to load metadata JSONL from {metadata_path}: {e}")
        return None, None

    if index.ntotal != len(metadata_map):
        logging.warning(f"Mismatch between index size ({index.ntotal}) and metadata size ({len(metadata_map)}). Ensure metadata file is correct.")

    return index, metadata_map

def search_kb(query, model, index, metadata_map, k=5):
    """Embeds the query and searches the FAISS index."""
    if index is None or metadata_map is None:
        logging.error("Knowledge base not loaded. Cannot search.")
        return []

    logging.info(f"Embedding query: '{query[:100]}...'")
    try:
        query_embedding = model.encode([query])
        query_embedding = np.array(query_embedding).astype('float32')
    except Exception as e:
        logging.error(f"Failed to embed query: {e}")
        return []

    logging.info(f"Searching index for top {k} results...")
    try:
        distances, indices = index.search(query_embedding, k)
        results = []
        if len(indices) > 0:
            for i, idx in enumerate(indices[0]):
                if idx != -1:
                    # metadata_map now holds the full object {chunk_id, text, metadata}
                    # The key `idx` directly corresponds to `chunk_id`
                    chunk_data = metadata_map.get(idx)
                    if chunk_data:
                        results.append({
                            'index_id': idx,
                            'distance': float(distances[0][i]),
                            'chunk_data': chunk_data # Pass the whole retrieved object
                        })
                    else:
                        logging.warning(f"Metadata not found for index {idx}.")
        logging.info(f"Found {len(results)} relevant chunks.")
        return results
    except Exception as e:
        logging.error(f"Error during FAISS search: {e}")
        return []

# --- Main Execution ---

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test the paper knowledge base.")
    parser.add_argument("query", type=str, help="The search query to run against the knowledge base.")
    parser.add_argument("-k", type=int, default=3, help="Number of results to retrieve (default: 3).")
    parser.add_argument("--kb_dir", type=str, default=DEFAULT_KB_DIR,
                        help=f"Path to the knowledge base directory (default: {DEFAULT_KB_DIR})")
    parser.add_argument("--index_file", type=str, default=DEFAULT_VECTOR_STORE_FILENAME,
                        help=f"Vector store filename (default: {DEFAULT_VECTOR_STORE_FILENAME})")
    parser.add_argument("--meta_file", type=str, default=DEFAULT_METADATA_FILENAME,
                         help=f"Metadata filename (JSONL) (default: {DEFAULT_METADATA_FILENAME})")

    args = parser.parse_args()

    # Use the provided or default KB directory and filenames
    kb_directory_to_use = args.kb_dir
    index_filename = args.index_file
    metadata_filename = args.meta_file

    # 1. Load Embedding Model
    logging.info(f"Loading embedding model: {EMBEDDING_MODEL_NAME}...")
    try:
        model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    except Exception as e:
        logging.error(f"Failed to load embedding model '{EMBEDDING_MODEL_NAME}': {e}")
        sys.exit(1)

    # 2. Load Knowledge Base
    index, metadata_map = load_knowledge_base(kb_directory_to_use, index_filename, metadata_filename)
    if index is None or metadata_map is None:
        sys.exit(1)

    # 3. Search
    search_results = search_kb(args.query, model, index, metadata_map, k=args.k)

    # 4. Print Results (Adjusted for new metadata structure)
    print(f"\n--- Search Results for: '{args.query}' ---")
    if not search_results:
        print("No relevant chunks found.")
    else:
        for i, result in enumerate(search_results):
            chunk_info = result['chunk_data'] # Contains text and metadata sub-dict
            meta = chunk_info.get('metadata', {}) # Safely get metadata sub-dict
            print(f"\nResult {i+1} (Distance: {result['distance']:.4f}):")
            print(f"  Source: {meta.get('source', 'N/A')}")
            print(f"  Title (heuristic): {meta.get('potential_title', 'N/A')}")
            # Pages might be 'unknown' now with Mathpix
            print(f"  Pages: {meta.get('pages', 'N/A')}") 
            print(f"  Paragraph Index Range: {meta.get('start_para_index', '?')} - {meta.get('end_para_index', '?')}")
            print(f"--- Chunk Text (Index: {result['index_id']}) --- ")
            print(chunk_info.get('text', '[Error retrieving text]')) # Get text from chunk_info
            print("---------------------------------")

    logging.info("Test script finished.") 