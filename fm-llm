#!/usr/bin/env python3
"""
FM-LLM Solver - Unified Command Line Interface

This script consolidates all entry points into a single, authoritative command:
- Replaces run_application.py, run_web_interface.py, modal_inference_app.py
- Preserves all GCP + Modal hybrid functionality
- Provides unified interface for all operations

Usage:
    fm-llm start web                    # Web interface only (hybrid mode)
    fm-llm start inference              # Inference API only
    fm-llm start hybrid                 # Web local + Modal inference
    fm-llm start full                   # Full local stack
    fm-llm deploy modal                 # Deploy to Modal
    fm-llm deploy hybrid                # Hybrid GCP + Modal deployment
    fm-llm generate "system desc"       # Direct certificate generation
    fm-llm kb build                     # Build knowledge base
    fm-llm test --unit --integration    # Run tests
    fm-llm status                       # System status
"""

import argparse
import asyncio
import os
import subprocess
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
import json

# Add project root to Python path
PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from utils.config_loader import load_config
    from fm_llm_solver.core.logging import get_logger, configure_logging
except ImportError:
    # Fallback for environments without full setup
    def load_config():
        return {"environment": {"mode": "development"}, "deployment": {"mode": "local"}}
    
    class FallbackLogger:
        def info(self, msg): print(f"INFO: {msg}")
        def warning(self, msg): print(f"WARNING: {msg}")
        def error(self, msg): print(f"ERROR: {msg}")
        def debug(self, msg): pass  # Silent in fallback mode
    
    def get_logger(name): return FallbackLogger()
    def configure_logging(**kwargs): pass

# Global logger
logger = get_logger(__name__)

class FMLLMCLIError(Exception):
    """Base exception for FM-LLM CLI errors."""
    pass

class FMLLMCommand:
    """Base class for FM-LLM commands."""
    
    def __init__(self, config: Dict):
        self.config = config
        self.project_root = PROJECT_ROOT
    
    def run_command(self, cmd: List[str], description: str, check: bool = True) -> subprocess.CompletedProcess:
        """Run a shell command with logging."""
        logger.info(f"üöÄ {description}")
        logger.info(f"Command: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(cmd, cwd=self.project_root, check=check, capture_output=True, text=True)
            if result.stdout:
                print(result.stdout)
            return result
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå {description} failed: {e}")
            if e.stderr:
                print(f"Error: {e.stderr}")
            raise FMLLMCLIError(f"{description} failed") from e

class StartCommand(FMLLMCommand):
    """Handle 'fm-llm start' commands - replaces run_application.py and run_web_interface.py"""
    
    def web(self, debug: bool = False, host: str = "127.0.0.1", port: int = 5000):
        """Start web interface only (hybrid mode default)."""
        logger.info("üåê Starting FM-LLM Web Interface (Hybrid Mode)")
        
        # Check if we're in hybrid mode
        deployment_mode = self.config.get("deployment", {}).get("mode", "local")
        
        if deployment_mode == "hybrid":
            logger.info("üì° Hybrid mode detected - web local, inference via Modal")
        
        env = os.environ.copy()
        env.update({
            "FLASK_ENV": "development" if debug else "production",
            "FLASK_DEBUG": "1" if debug else "0",
            "FM_LLM_ENV": self.config.get("environment", {}).get("mode", "development"),
            "DEPLOYMENT_MODE": deployment_mode
        })
        
        # Start web interface
        if debug:
            cmd = [sys.executable, "web_interface/app.py", "--host", host, "--port", str(port), "--debug"]
        else:
            cmd = [sys.executable, "-m", "flask", "--app", "web_interface.app", "run", 
                   "--host", host, "--port", str(port)]
        
        try:
            subprocess.run(cmd, env=env, cwd=self.project_root)
        except KeyboardInterrupt:
            logger.info("üõë Web interface stopped by user")
    
    def inference(self, gpu: bool = True, host: str = "127.0.0.1", port: int = 8000):
        """Start inference API only."""
        logger.info("üß† Starting FM-LLM Inference API")
        
        env = os.environ.copy()
        if gpu:
            env["CUDA_VISIBLE_DEVICES"] = self.config.get("performance", {}).get("cuda", {}).get("visible_devices", "0")
        
        cmd = [sys.executable, "-m", "uvicorn", "inference_api.main:app", 
               "--host", host, "--port", str(port)]
        
        if self.config.get("environment", {}).get("debug", False):
            cmd.append("--reload")
        
        try:
            subprocess.run(cmd, env=env, cwd=self.project_root)
        except KeyboardInterrupt:
            logger.info("üõë Inference API stopped by user")
    
    def hybrid(self, debug: bool = False):
        """Start hybrid mode: web local + Modal inference."""
        logger.info("üîó Starting Hybrid Mode: Web Local + Modal Inference")
        
        # First ensure Modal is deployed
        deploy_cmd = DeployCommand(self.config)
        deploy_cmd.modal()
        
        # Set hybrid mode
        os.environ["DEPLOYMENT_MODE"] = "hybrid"
        
        # Start web interface
        self.web(debug=debug)
    
    def full(self, debug: bool = False, host: str = "127.0.0.1"):
        """Start full local stack (web + inference)."""
        logger.info("üöÄ Starting Full Local Stack")
        
        import threading
        
        os.environ["DEPLOYMENT_MODE"] = "local"
        
        # Start inference API in background
        inference_thread = threading.Thread(
            target=self.inference, 
            kwargs={"host": host, "port": 8000},
            daemon=True
        )
        inference_thread.start()
        
        # Wait a moment for inference to start
        time.sleep(3)
        
        # Start web interface in foreground
        self.web(debug=debug, host=host, port=5000)

class DeployCommand(FMLLMCommand):
    """Handle 'fm-llm deploy' commands - includes Modal functionality."""
    
    def modal(self, app_name: str = "fm-llm-solver"):
        """Deploy inference to Modal - replaces modal_inference_app.py functionality."""
        logger.info("‚òÅÔ∏è Deploying to Modal")
        
        # Check if Modal is installed and authenticated
        try:
            self.run_command(["modal", "token", "current"], "Checking Modal authentication", check=False)
        except FMLLMCLIError:
            logger.error("‚ùå Modal not authenticated. Run: modal token new")
            return False
        
        # Deploy the Modal app
        try:
            self.run_command(["modal", "deploy", "modal_inference_app.py"], "Deploying to Modal")
            logger.info("‚úÖ Modal deployment successful!")
            
            # Get the deployed URL
            result = self.run_command(["modal", "app", "list"], "Getting Modal URL", check=False)
            # Parse URL from output and update config if needed
            
            return True
        except FMLLMCLIError:
            logger.error("‚ùå Modal deployment failed")
            return False
    
    def hybrid(self):
        """Deploy hybrid GCP + Modal architecture."""
        logger.info("üåê Deploying Hybrid GCP + Modal Architecture")
        
        # Run the existing hybrid deployment script
        try:
            self.run_command(["bash", "deployment/deploy_hybrid.sh"], "Deploying hybrid architecture")
            logger.info("‚úÖ Hybrid deployment successful!")
            return True
        except FMLLMCLIError:
            logger.error("‚ùå Hybrid deployment failed")
            return False
    
    def gcp(self, environment: str = "production"):
        """Deploy to GCP Kubernetes."""
        logger.info(f"‚òÅÔ∏è Deploying to GCP ({environment})")
        
        config_file = f"deployment/environments/{environment}/complete-{environment}-deployment.yaml"
        if not Path(config_file).exists():
            config_file = f"deployment/environments/{environment}/production-web-deployment.yaml"
        
        try:
            self.run_command(["kubectl", "apply", "-f", config_file], f"Deploying to GCP {environment}")
            logger.info("‚úÖ GCP deployment successful!")
            return True
        except FMLLMCLIError:
            logger.error("‚ùå GCP deployment failed")
            return False

class GenerateCommand(FMLLMCommand):
    """Handle direct certificate generation."""
    
    def certificate(self, system_description: str, model: str = "base", output: Optional[str] = None):
        """Generate barrier certificate directly."""
        logger.info("üßÆ Generating barrier certificate")
        
        try:
            # Import generation modules
            from fm_llm_solver.services.certificate_generator import CertificateGenerator
            from fm_llm_solver.core.types import SystemDescription, SystemType
            
            # Create generator
            generator = CertificateGenerator(self.config)
            
            # Parse system description (simplified for CLI)
            system = SystemDescription(
                dynamics={"x": system_description},
                initial_set="x <= 1",
                unsafe_set="x >= 10", 
                system_type=SystemType.CONTINUOUS,
            )
            
            # Generate certificate
            result = generator.generate(system)
            
            # Output result
            if output:
                with open(output, 'w') as f:
                    f.write(str(result))
                logger.info(f"‚úÖ Certificate saved to {output}")
            else:
                print("\nüßÆ Generated Certificate:")
                print(result)
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Generation failed: {e}")
            return False

class KnowledgeBaseCommand(FMLLMCommand):
    """Handle knowledge base operations."""
    
    def build(self, kb_type: str = "unified", rebuild: bool = False):
        """Build knowledge base."""
        logger.info(f"üìö Building {kb_type} knowledge base")
        
        cmd = [sys.executable, "knowledge_base/knowledge_base_builder.py", "--type", kb_type]
        if rebuild:
            cmd.append("--rebuild")
        
        try:
            self.run_command(cmd, "Building knowledge base")
            logger.info("‚úÖ Knowledge base built successfully!")
            return True
        except FMLLMCLIError:
            logger.error("‚ùå Knowledge base build failed")
            return False

class TestCommand(FMLLMCommand):
    """Handle testing operations - consolidates 9+ test runners into unified interface."""
    
    def __init__(self, config: Dict):
        super().__init__(config)
        self.test_results = {}
        self.start_time = None
        
    def _detect_test_environment(self) -> Dict[str, Any]:
        """Detect testing environment capabilities."""
        import platform
        
        env_info = {
            "platform": platform.system(),
            "python_version": platform.python_version(),
            "cpu_cores": 4,  # Default fallback
            "memory_gb": 8,  # Default fallback
            "gpu_available": False,
            "gpu_name": None,
            "gpu_memory_gb": None,
            "environment_type": "unknown"
        }
        
        # Try to get accurate system info with psutil
        try:
            import psutil
            env_info["cpu_cores"] = psutil.cpu_count()
            env_info["memory_gb"] = round(psutil.virtual_memory().total / (1024**3), 1)
        except ImportError:
            # Use fallback values if psutil not available
            import multiprocessing
            env_info["cpu_cores"] = multiprocessing.cpu_count()
            logger.debug("psutil not available, using fallback system detection")
        
        # Check GPU availability
        try:
            import torch
            if torch.cuda.is_available():
                env_info["gpu_available"] = True
                env_info["gpu_name"] = torch.cuda.get_device_name(0)
                env_info["gpu_memory_gb"] = round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 1)
        except ImportError:
            pass
        
        # Determine environment type based on capabilities
        if env_info["gpu_available"] and env_info["memory_gb"] >= 16:
            env_info["environment_type"] = "desktop"  # High-powered development
        elif env_info["memory_gb"] >= 8:
            env_info["environment_type"] = "laptop"   # Moderate development
        else:
            env_info["environment_type"] = "minimal"  # CI/CD or constrained
            
        return env_info
    
    def _run_pytest_command(self, test_paths: List[str], args: List[str] = None, description: str = "") -> bool:
        """Run pytest with specified paths and arguments."""
        if not test_paths:
            return True
            
        cmd = [sys.executable, "-m", "pytest"] + test_paths
        if args:
            cmd.extend(args)
        
        try:
            result = self.run_command(cmd, description or f"Running tests: {', '.join(test_paths)}")
            return result.returncode == 0
        except FMLLMCLIError:
            return False
    
    def _run_quick_check(self) -> bool:
        """Run quick system check - replaces tests/run_tests.py --quick."""
        logger.info("üöÄ Running Quick System Check")
        
        # Check core imports
        components = [
            ("utils.config_loader", "Configuration System"),
            ("utils.hierarchical_config_loader", "Hierarchical Config"),
            ("fm_llm_solver.core", "Core Services"),
            ("fm_llm_solver.services", "Business Services"),
            ("web_interface.app", "Web Interface"),
            ("torch", "PyTorch (GPU Support)"),
        ]
        
        all_good = True
        for module, name in components:
            try:
                __import__(module)
                logger.info(f"  ‚úÖ {name}")
            except ImportError as e:
                logger.error(f"  ‚ùå {name}: {e}")
                all_good = False
        
        # Check GPU if available
        env_info = self._detect_test_environment()
        if env_info["gpu_available"]:
            logger.info(f"  üéÆ GPU: {env_info['gpu_name']} ({env_info['gpu_memory_gb']}GB)")
        else:
            logger.info("  üéÆ GPU: Not available")
        
        logger.info(f"  üíª Environment: {env_info['environment_type']} ({env_info['memory_gb']}GB RAM)")
        
        if all_good:
            logger.info("‚úÖ Quick check passed!")
        else:
            logger.error("‚ùå Quick check failed!")
        
        return all_good
    
    def _run_unit_tests(self, fast: bool = False) -> bool:
        """Run unit tests - consolidates unit testing from all runners."""
        logger.info("üß™ Running Unit Tests")
        
        args = [
            "-v",
            "--tb=short",
            "-m", "unit or not integration and not performance and not slow"
        ]
        
        if fast:
            args.extend(["-x", "--disable-warnings"])
        
        # Add coverage if requested
        if not fast:
            args.extend([
                "--cov=fm_llm_solver",
                "--cov=utils",
                "--cov-report=term-missing",
                "--cov-report=html:test_results/coverage_html"
            ])
        
        return self._run_pytest_command(["tests/unit/"], args, "Unit Tests")
    
    def _run_integration_tests(self, fast: bool = False) -> bool:
        """Run integration tests - consolidates integration testing."""
        logger.info("üîó Running Integration Tests")
        
        args = ["-v", "--tb=short", "-m", "integration or not unit and not performance"]
        
        if fast:
            args.extend(["-x", "--disable-warnings"])
        
        return self._run_pytest_command(["tests/integration/"], args, "Integration Tests")
    
    def _run_performance_tests(self) -> bool:
        """Run performance tests - consolidates performance testing."""
        logger.info("‚ö° Running Performance Tests")
        
        args = ["-v", "--tb=short", "-m", "performance", "--durations=10"]
        
        return self._run_pytest_command(["tests/performance/"], args, "Performance Tests")
    
    def _run_gpu_tests(self) -> bool:
        """Run GPU-accelerated tests - consolidates GPU testing."""
        env_info = self._detect_test_environment()
        
        if not env_info["gpu_available"]:
            logger.warning("üéÆ GPU not available, skipping GPU tests")
            return True
        
        logger.info(f"üéÆ Running GPU Tests on {env_info['gpu_name']}")
        
        args = [
            "-v", 
            "--tb=short", 
            "-m", "gpu or cuda",
            f"--gpu-memory={env_info['gpu_memory_gb']}GB"
        ]
        
        return self._run_pytest_command(["tests/"], args, "GPU Tests")
    
    def _run_phase2_tests(self) -> bool:
        """Run Phase 2 specific tests."""
        logger.info("üéØ Running Phase 2 Tests")
        
        args = ["-v", "--tb=short"]
        
        return self._run_pytest_command(["tests/phase2/"], args, "Phase 2 Tests")
    
    def _run_e2e_tests(self) -> bool:
        """Run end-to-end tests."""
        logger.info("üåê Running End-to-End Tests")
        
        args = ["-v", "--tb=short", "-m", "e2e"]
        
        return self._run_pytest_command(["tests/e2e/"], args, "End-to-End Tests")
    
    def _run_security_tests(self) -> bool:
        """Run security tests."""
        logger.info("üîí Running Security Tests")
        
        args = ["-v", "--tb=short", "-m", "security"]
        
        return self._run_pytest_command(["tests/"], args, "Security Tests")
    
    def _run_benchmark_tests(self) -> bool:
        """Run benchmark tests."""
        logger.info("üìä Running Benchmark Tests")
        
        args = ["-v", "--tb=short", "--benchmark-only"]
        
        return self._run_pytest_command(["tests/benchmarks/"], args, "Benchmark Tests")
    
    def _generate_test_report(self, results: Dict[str, bool], env_info: Dict[str, Any]) -> None:
        """Generate comprehensive test report."""
        total_tests = len(results)
        passed_tests = sum(1 for success in results.values() if success)
        failed_tests = total_tests - passed_tests
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "environment": env_info,
            "summary": {
                "total_test_suites": total_tests,
                "passed_suites": passed_tests,
                "failed_suites": failed_tests,
                "success_rate": round(success_rate, 1)
            },
            "results": results,
            "recommendations": []
        }
        
        # Add recommendations based on results
        if success_rate >= 90:
            report["recommendations"].append("‚úÖ Excellent! All systems operational.")
        elif success_rate >= 75:
            report["recommendations"].append("‚ö†Ô∏è Good overall, investigate failed tests.")
        else:
            report["recommendations"].append("‚ùå Multiple issues detected, review failed tests.")
        
        if not env_info["gpu_available"]:
            report["recommendations"].append("üí° Consider GPU setup for enhanced performance.")
        
        # Save report
        os.makedirs("test_results", exist_ok=True)
        with open("test_results/unified_test_report.json", "w") as f:
            json.dump(report, f, indent=2)
        
        # Print summary
        print("\n" + "="*60)
        print("üß™ FM-LLM Test Suite Results")
        print("="*60)
        print(f"üìä Test Suites: {passed_tests}/{total_tests} passed ({success_rate:.1f}%)")
        print(f"üíª Environment: {env_info['environment_type']} ({env_info['platform']})")
        if env_info["gpu_available"]:
            print(f"üéÆ GPU: {env_info['gpu_name']}")
        print(f"üêç Python: {env_info['python_version']}")
        
        print("\nüìã Results by Test Suite:")
        for suite_name, success in results.items():
            status = "‚úÖ PASS" if success else "‚ùå FAIL"
            print(f"  {status} {suite_name}")
        
        print(f"\nüíæ Full report saved to: test_results/unified_test_report.json")
        
        for rec in report["recommendations"]:
            print(f"   {rec}")
        print("="*60)
    
    def run(self, unit: bool = False, integration: bool = False, performance: bool = False, 
            gpu: bool = False, phase2: bool = False, e2e: bool = False, security: bool = False,
            benchmark: bool = False, quick: bool = False, all_tests: bool = False, 
            fast: bool = False, coverage: bool = False, adaptive: bool = False):
        """Run unified test suite - consolidates all 9+ test runners."""
        
        self.start_time = time.time()
        
        # Environment detection
        env_info = self._detect_test_environment()
        logger.info(f"üîç Environment detected: {env_info['environment_type']} ({env_info['platform']})")
        
        # Determine test strategy
        if quick:
            return self._run_quick_check()
        
        if adaptive:
            # Adaptive testing based on environment
            if env_info["environment_type"] == "desktop":
                # High-powered: run everything
                unit = integration = performance = gpu = phase2 = True
            elif env_info["environment_type"] == "laptop":
                # Moderate: essential tests
                unit = integration = phase2 = True
                gpu = env_info["gpu_available"]
            else:
                # Minimal: just unit tests
                unit = True
        
        if all_tests:
            unit = integration = performance = e2e = security = benchmark = phase2 = True
            gpu = env_info["gpu_available"]
        
        # If no specific tests selected, default to unit + integration
        if not any([unit, integration, performance, gpu, phase2, e2e, security, benchmark]):
            unit = integration = True
        
        # Run selected test suites
        results = {}
        
        if unit:
            results["Unit Tests"] = self._run_unit_tests(fast=fast)
        
        if integration:
            results["Integration Tests"] = self._run_integration_tests(fast=fast)
        
        if performance:
            results["Performance Tests"] = self._run_performance_tests()
        
        if gpu:
            results["GPU Tests"] = self._run_gpu_tests()
        
        if phase2:
            results["Phase 2 Tests"] = self._run_phase2_tests()
        
        if e2e:
            results["End-to-End Tests"] = self._run_e2e_tests()
        
        if security:
            results["Security Tests"] = self._run_security_tests()
        
        if benchmark:
            results["Benchmark Tests"] = self._run_benchmark_tests()
        
        # Generate comprehensive report
        self._generate_test_report(results, env_info)
        
        # Return overall success
        total_duration = time.time() - self.start_time
        overall_success = all(results.values())
        
        if overall_success:
            logger.info(f"‚úÖ All tests passed! ({total_duration:.1f}s)")
        else:
            logger.error(f"‚ùå Some tests failed! ({total_duration:.1f}s)")
        
        return overall_success

class StatusCommand(FMLLMCommand):
    """Handle status operations."""
    
    def show(self):
        """Show system status."""
        logger.info("üìä FM-LLM System Status")
        
        print("\nüèóÔ∏è FM-LLM Solver Status")
        print("=" * 50)
        
        # Check Python environment
        print(f"üêç Python: {sys.version.split()[0]} ({sys.executable})")
        print(f"üìÅ Project Root: {self.project_root}")
        
        # Check configuration
        deployment_mode = self.config.get("deployment", {}).get("mode", "unknown")
        environment = self.config.get("environment", {}).get("mode", "unknown")
        print(f"‚öôÔ∏è Environment: {environment}")
        print(f"üöÄ Deployment Mode: {deployment_mode}")
        
        # Check GPU availability
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                print(f"üéÆ GPU: Available ({gpu_count} devices, {gpu_name})")
            else:
                print("üéÆ GPU: Not available")
        except ImportError:
            print("üéÆ GPU: PyTorch not installed")
        
        # Check Modal status if in hybrid mode
        if deployment_mode == "hybrid":
            try:
                result = subprocess.run(["modal", "app", "list"], capture_output=True, text=True)
                if result.returncode == 0:
                    print("‚òÅÔ∏è Modal: Connected")
                else:
                    print("‚òÅÔ∏è Modal: Not connected")
            except FileNotFoundError:
                print("‚òÅÔ∏è Modal: Not installed")
        
        # Check services
        print("\nüîß Services:")
        
        # Check if web interface is running
        try:
            import requests
            response = requests.get("http://localhost:5000/health", timeout=2)
            print("üåê Web Interface: Running ‚úÖ")
        except:
            print("üåê Web Interface: Not running ‚è∏Ô∏è")
        
        # Check if inference API is running
        try:
            import requests
            response = requests.get("http://localhost:8000/health", timeout=2)
            print("üß† Inference API: Running ‚úÖ")
        except:
            print("üß† Inference API: Not running ‚è∏Ô∏è")
        
        print("\n‚úÖ Status check complete!")

def main():
    """Main CLI entry point."""
    # Configure logging
    configure_logging(level="INFO", console=True)
    
    # Load configuration
    try:
        config = load_config()
    except Exception as e:
        logger.warning(f"Could not load full config: {e}")
        config = {"environment": {"mode": "development"}, "deployment": {"mode": "local"}}
    
    parser = argparse.ArgumentParser(
        description="FM-LLM Solver - Unified Command Line Interface",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Start command
    start_parser = subparsers.add_parser("start", help="Start FM-LLM services")
    start_subparsers = start_parser.add_subparsers(dest="service", help="Service to start")
    
    # Start web
    web_parser = start_subparsers.add_parser("web", help="Start web interface (hybrid mode)")
    web_parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    web_parser.add_argument("--host", default="127.0.0.1", help="Host to bind to")
    web_parser.add_argument("--port", type=int, default=5000, help="Port to bind to")
    
    # Start inference
    inference_parser = start_subparsers.add_parser("inference", help="Start inference API")
    inference_parser.add_argument("--no-gpu", action="store_true", help="Disable GPU")
    inference_parser.add_argument("--host", default="127.0.0.1", help="Host to bind to")
    inference_parser.add_argument("--port", type=int, default=8000, help="Port to bind to")
    
    # Start hybrid
    hybrid_parser = start_subparsers.add_parser("hybrid", help="Start hybrid mode")
    hybrid_parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    
    # Start full
    full_parser = start_subparsers.add_parser("full", help="Start full local stack")
    full_parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    full_parser.add_argument("--host", default="127.0.0.1", help="Host to bind to")
    
    # Deploy command
    deploy_parser = subparsers.add_parser("deploy", help="Deploy FM-LLM services")
    deploy_subparsers = deploy_parser.add_subparsers(dest="target", help="Deployment target")
    
    deploy_subparsers.add_parser("modal", help="Deploy to Modal")
    deploy_subparsers.add_parser("hybrid", help="Deploy hybrid GCP + Modal")
    
    gcp_parser = deploy_subparsers.add_parser("gcp", help="Deploy to GCP")
    gcp_parser.add_argument("--env", default="production", help="Environment to deploy")
    
    # Generate command
    generate_parser = subparsers.add_parser("generate", help="Generate barrier certificate")
    generate_parser.add_argument("system_description", help="System description")
    generate_parser.add_argument("--model", default="base", help="Model to use")
    generate_parser.add_argument("--output", "-o", help="Output file")
    
    # Knowledge base command
    kb_parser = subparsers.add_parser("kb", help="Knowledge base operations")
    kb_subparsers = kb_parser.add_subparsers(dest="kb_operation", help="KB operation")
    
    build_parser = kb_subparsers.add_parser("build", help="Build knowledge base")
    build_parser.add_argument("--type", default="unified", choices=["unified", "discrete", "continuous"])
    build_parser.add_argument("--rebuild", action="store_true", help="Force rebuild")
    
    # Test command
    test_parser = subparsers.add_parser("test", help="Run unified test suite")
    test_parser.add_argument("--unit", action="store_true", help="Run unit tests")
    test_parser.add_argument("--integration", action="store_true", help="Run integration tests") 
    test_parser.add_argument("--performance", action="store_true", help="Run performance tests")
    test_parser.add_argument("--gpu", action="store_true", help="Run GPU tests")
    test_parser.add_argument("--phase2", action="store_true", help="Run Phase 2 tests")
    test_parser.add_argument("--e2e", action="store_true", help="Run end-to-end tests")
    test_parser.add_argument("--security", action="store_true", help="Run security tests")
    test_parser.add_argument("--benchmark", action="store_true", help="Run benchmark tests")
    test_parser.add_argument("--quick", action="store_true", help="Run quick system check")
    test_parser.add_argument("--all", action="store_true", help="Run all test suites")
    test_parser.add_argument("--fast", action="store_true", help="Run tests in fast mode")
    test_parser.add_argument("--coverage", action="store_true", help="Generate coverage reports")
    test_parser.add_argument("--adaptive", action="store_true", help="Use adaptive testing based on environment")
    
    # Status command
    subparsers.add_parser("status", help="Show system status")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    try:
        # Route to appropriate command handler
        if args.command == "start":
            cmd = StartCommand(config)
            if args.service == "web":
                cmd.web(debug=args.debug, host=args.host, port=args.port)
            elif args.service == "inference":
                cmd.inference(gpu=not args.no_gpu, host=args.host, port=args.port)
            elif args.service == "hybrid":
                cmd.hybrid(debug=args.debug)
            elif args.service == "full":
                cmd.full(debug=args.debug, host=args.host)
            else:
                start_parser.print_help()
                
        elif args.command == "deploy":
            cmd = DeployCommand(config)
            if args.target == "modal":
                cmd.modal()
            elif args.target == "hybrid":
                cmd.hybrid()
            elif args.target == "gcp":
                cmd.gcp(environment=args.env)
            else:
                deploy_parser.print_help()
                
        elif args.command == "generate":
            cmd = GenerateCommand(config)
            cmd.certificate(args.system_description, model=args.model, output=args.output)
            
        elif args.command == "kb":
            cmd = KnowledgeBaseCommand(config)
            if args.kb_operation == "build":
                cmd.build(kb_type=args.type, rebuild=args.rebuild)
            else:
                kb_parser.print_help()
                
        elif args.command == "test":
            cmd = TestCommand(config)
            cmd.run(unit=args.unit, integration=args.integration, performance=args.performance,
                   gpu=args.gpu, phase2=args.phase2, e2e=args.e2e, security=args.security,
                   benchmark=args.benchmark, quick=args.quick, all_tests=args.all,
                   fast=args.fast, coverage=args.coverage, adaptive=args.adaptive)
                   
        elif args.command == "status":
            cmd = StatusCommand(config)
            cmd.show()
        
        return 0
        
    except FMLLMCLIError as e:
        logger.error(f"Command failed: {e}")
        return 1
    except KeyboardInterrupt:
        logger.info("Command interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 