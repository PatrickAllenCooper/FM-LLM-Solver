# FM-LLM Solver - Base Configuration
# This file contains the foundational configuration shared across all environments
# Environment-specific files override these settings as needed

# ============================================================================
# Project Metadata
# ============================================================================
project:
  name: "FM-LLM Solver"
  version: "2.0"
  description: "Barrier Certificate Generation using Large Language Models"

# ============================================================================
# Core System Settings
# ============================================================================
environment:
  mode: development  # Override in environment files: development, production, testing
  debug: false       # Override in development
  log_level: INFO    # Override per environment

# ============================================================================
# External API Configuration (Template)
# ============================================================================
secrets:
  # Web Interface Security (MUST be overridden in production)
  secret_key: "dev-secret-key-change-in-production"
  jwt_secret: "jwt-dev-secret"
  
  # External API Keys (set via environment variables)
  mathpix_app_id: "${ENV:MATHPIX_APP_ID:}"
  mathpix_app_key: "${ENV:MATHPIX_APP_KEY:}"
  unpaywall_email: "${ENV:UNPAYWALL_EMAIL:}"
  semantic_scholar_api_key: "${ENV:SEMANTIC_SCHOLAR_API_KEY:}"
  huggingface_token: "${ENV:HUGGINGFACE_TOKEN:}"

# ============================================================================
# Database Configuration
# ============================================================================
database:
  # Default to SQLite for development, override with PostgreSQL for production
  url: "sqlite:///instance/fmllm.db"
  
  # Connection pool settings (primarily for PostgreSQL)
  pool_size: 10
  max_overflow: 20
  pool_timeout: 30
  pool_recycle: 3600
  
  # SQLAlchemy settings
  track_modifications: false
  echo: false  # Override to true in development

# ============================================================================
# Cache Configuration
# ============================================================================
cache:
  enabled: true
  url: "redis://localhost:6379/0"
  default_timeout: 3600
  key_prefix: "fmllm:"

# ============================================================================
# Web Interface Configuration
# ============================================================================
web:
  # Server settings
  host: "127.0.0.1"
  port: 5000
  
  # Security settings
  csrf_enabled: true
  session_timeout: 3600
  max_content_length: 16777216  # 16MB
  
  # User management
  enable_registration: true
  enable_api_keys: true
  default_subscription: basic
  
  # Rate limiting
  rate_limit:
    enabled: true
    default: "100 per hour"
    api: "1000 per hour" 
    burst: "10 per minute"

# ============================================================================
# Inference & Model Configuration
# ============================================================================
inference:
  # API settings
  host: "127.0.0.1"
  port: 8000
  timeout: 300
  
  # External inference API (for hybrid deployments)
  api_url: "${ENV:INFERENCE_API_URL:}"
  
  # Default model configuration
  default_model: "base"

# ============================================================================
# Model Configurations
# ============================================================================
models:
  base:
    name: "Qwen/Qwen2.5-14B-Instruct"
    barrier_type: "discrete"
    max_new_tokens: 512
    temperature: 0.1
    top_p: 0.95
    do_sample: true
    device_map: auto
    torch_dtype: bfloat16
    
  large:
    name: "Qwen/Qwen2.5-72B-Instruct"
    barrier_type: "continuous"
    max_new_tokens: 768
    temperature: 0.05
    top_p: 0.9
    
  finetuned:
    name: "qwen2.5-14b-barrier-ft"
    barrier_type: "discrete"
    use_peft: true
    adapter_path: "output/finetuning_results/checkpoint-best"

# ============================================================================
# Knowledge Base & RAG Configuration
# ============================================================================
knowledge_base:
  # Embedding model
  embedding_model: "all-mpnet-base-v2"
  
  # Processing settings
  chunk_size: 1000
  chunk_overlap: 150
  batch_size: 32
  
  # RAG settings
  default_k: 3
  max_k: 10
  similarity_threshold: 0.7
  
  # Knowledge base types
  types:
    discrete:
      output_dir: kb_data_discrete
      vector_store: paper_index_discrete.faiss
      metadata_file: paper_metadata_discrete.jsonl
      
    continuous:
      output_dir: kb_data_continuous  
      vector_store: paper_index_continuous.faiss
      metadata_file: paper_metadata_continuous.jsonl
      
    unified:
      output_dir: kb_data
      vector_store: paper_index_mathpix.faiss
      metadata_file: paper_metadata_mathpix.jsonl

# ============================================================================
# Data Processing & Paths
# ============================================================================
paths:
  # Core directories
  data_dir: data
  output_dir: output
  logs_dir: logs
  cache_dir: cache
  models_dir: models
  
  # Input data
  pdf_input_dir: "${paths.data_dir}/fetched_papers"
  user_ids_csv: "${paths.data_dir}/user_ids.csv"
  benchmark_file: "${paths.data_dir}/benchmark_systems.json"
  
  # Fine-tuning data
  ft_manual_data: "${paths.data_dir}/ft_manual_data.jsonl"
  ft_extracted_data: "${paths.data_dir}/ft_extracted_data_verified.jsonl"
  ft_combined_data: "${paths.data_dir}/ft_data_combined.jsonl"
  ft_discrete_data: "${paths.data_dir}/ft_data_discrete.jsonl"
  ft_continuous_data: "${paths.data_dir}/ft_data_continuous.jsonl"
  ft_output_dir: "${paths.output_dir}/finetuning_results"
  
  # Evaluation
  eval_results: "${paths.output_dir}/evaluation_results.csv"

# ============================================================================
# External Data Fetching
# ============================================================================
data_fetching:
  # Rate limiting for external APIs
  sleep_time_scholarly: 3
  sleep_time_api: 1.5
  sleep_time_retry: 5
  max_retries: 2
  
  # Request settings
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
  publication_limit_per_author: 50
  
  # Mathpix settings
  mathpix:
    poll_max_wait_sec: 600
    poll_interval: 10

# ============================================================================
# Performance & Hardware
# ============================================================================
performance:
  # GPU settings
  cuda:
    visible_devices: "${ENV:CUDA_VISIBLE_DEVICES:0}"
    memory_config: "${ENV:PYTORCH_CUDA_ALLOC_CONF:max_split_size_mb:512}"
    
  # CPU settings
  cpu_workers: 4
  max_memory: 16  # GB
  
  # Caching
  model_cache_size: 2  # Number of models to keep in memory
  enable_model_offload: true

# ============================================================================
# Monitoring & Logging
# ============================================================================
monitoring:
  # Prometheus metrics
  metrics_enabled: true
  metrics_port: 9090
  
  # Health checks
  health_check_interval: 30
  health_check_timeout: 10
  
  # Performance tracking
  track_generation_time: true
  track_memory_usage: true
  track_gpu_usage: true

# ============================================================================
# Security Settings
# ============================================================================
security:
  # CORS
  cors_origins: "${ENV:CORS_ORIGINS:*}"
  
  # Content Security Policy
  csp_enabled: true
  
  # API Security
  api_key_length: 32
  api_rate_limit: "1000 per hour"
  
  # Password policy
  password_min_length: 8
  password_require_special: true

# ============================================================================
# Feature Flags
# ============================================================================
features:
  # Experimental features
  enable_conversation_mode: true
  enable_batch_processing: false
  enable_model_comparison: true
  enable_verification_service: true
  
  # UI features
  enable_dark_mode: true
  enable_advanced_settings: true
  enable_export_formats: ["json", "latex", "pdf"]

# ============================================================================
# Deployment Configuration (Base)
# ============================================================================
deployment:
  mode: local  # Override in environment files: local, hybrid, cloud
  
  # Service configuration
  services:
    web:
      enabled: true
      workers: 4
      timeout: 120
      
    inference:
      enabled: true
      workers: 1
      timeout: 300 