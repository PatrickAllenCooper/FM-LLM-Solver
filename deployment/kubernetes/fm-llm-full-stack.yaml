# FM-LLM Solver Full Stack Production Deployment
# This configuration deploys the complete application including:
# - Web Interface (Flask)
# - Inference API (FastAPI)
# - PostgreSQL Database (Cloud SQL)
# - Redis Cache
# - Load Balancing and Auto-scaling

apiVersion: v1
kind: Namespace
metadata:
  name: fm-llm-prod
  labels:
    name: fm-llm-prod
    cost-center: fm-llm-solver
---
# Web Interface Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fm-llm-web
  namespace: fm-llm-prod
  labels:
    app: fm-llm-web
    component: web-interface
    version: production
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: fm-llm-web
  template:
    metadata:
      labels:
        app: fm-llm-web
        component: web-interface
        version: production
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "5000"
        prometheus.io/path: "/metrics"
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      containers:
      - name: web-app
        image: us-central1-docker.pkg.dev/fmgen-net-production/fm-llm-repo/fm-llm-web:latest
        ports:
        - containerPort: 5000
          name: http
        env:
        - name: DEPLOYMENT_MODE
          value: "gcp-production"
        - name: ENVIRONMENT
          value: "production"
        - name: DOMAIN
          value: "fmgen.net"
        - name: INFERENCE_API_URL
          value: "http://fm-llm-inference-service:8000"
        - name: DATABASE_URL
          value: "postgresql://$(DB_USERNAME):$(DB_PASSWORD)@127.0.0.1:5432/$(DB_DATABASE)"
        - name: REDIS_URL
          value: "redis://${REDIS_HOST}:${REDIS_PORT}/0"
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: secret-key
        - name: ENCRYPTION_KEY
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: encryption-key
        - name: JWT_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: jwt-secret
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
        - name: DB_DATABASE
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: database
        - name: FLASK_ENV
          value: "production"
        - name: FLASK_DEBUG
          value: "false"
        - name: USER_QUOTA_ENABLED
          value: "true"
        - name: COST_TRACKING_ENABLED
          value: "true"
        - name: AUTO_SCALING_ENABLED
          value: "true"
        resources:
          requests:
            memory: "512Mi"
            cpu: "300m"
            ephemeral-storage: "1Gi"
          limits:
            memory: "1Gi"
            cpu: "800m"
            ephemeral-storage: "2Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
        - name: instance
          mountPath: /app/instance
      - name: cloudsql-proxy
        image: gcr.io/cloudsql-docker/gce-proxy:1.33.2
        command:
        - "/cloud_sql_proxy"
        - "-instances=${SQL_CONNECTION_NAME}=tcp:5432"
        - "-credential_file=/secrets/cloudsql/key.json"
        - "-log_debug_stdout"
        securityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cloudsql-key
          mountPath: /secrets/cloudsql
          readOnly: true
        - name: tmp
          mountPath: /tmp
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
      volumes:
      - name: cloudsql-key
        secret:
          secretName: cloudsql-key
          defaultMode: 0400
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir:
          sizeLimit: 1Gi
      - name: instance
        emptyDir:
          sizeLimit: 500Mi
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - fm-llm-web
              topologyKey: kubernetes.io/hostname
---
# Inference API Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fm-llm-inference
  namespace: fm-llm-prod
  labels:
    app: fm-llm-inference
    component: inference-api
    version: production
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: fm-llm-inference
  template:
    metadata:
      labels:
        app: fm-llm-inference
        component: inference-api
        version: production
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      initContainers:
      - name: model-downloader
        image: google/cloud-sdk:alpine
        command:
        - "/bin/sh"
        - "-c"
        - |
          echo "ðŸ“¥ Downloading models and knowledge base..."
          if [ -n "$MODELS_BUCKET" ]; then
            echo "Downloading models from $MODELS_BUCKET..."
            mkdir -p /shared/models
            gsutil -m rsync -r gs://$MODELS_BUCKET/ /shared/models/
            echo "âœ… Models downloaded"
          fi
          if [ -n "$KB_BUCKET" ]; then
            echo "Downloading knowledge base from $KB_BUCKET..."
            mkdir -p /shared/knowledge_base
            gsutil -m rsync -r gs://$KB_BUCKET/ /shared/knowledge_base/
            echo "âœ… Knowledge base downloaded"
          fi
          echo "ðŸŽ¯ Download complete!"
        env:
        - name: MODELS_BUCKET
          value: "fm-llm-models-fmgen-net-production"
        - name: KB_BUCKET
          value: "fm-llm-knowledge-base-fmgen-net-production"
        resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 200m
            memory: 512Mi
        volumeMounts:
        - name: shared-data
          mountPath: /shared
        securityContext:
          runAsUser: 1000
          runAsGroup: 2000
      containers:
      - name: inference-api
        image: us-central1-docker.pkg.dev/fmgen-net-production/fm-llm-repo/fm-llm-inference:latest
        ports:
        - containerPort: 8000
          name: http
        env:
        - name: DEPLOYMENT_MODE
          value: "gcp-production"
        - name: ENVIRONMENT
          value: "production"
        - name: REDIS_URL
          value: "redis://${REDIS_HOST}:${REDIS_PORT}/1"
        - name: CONFIG_PATH
          value: "/app/config/config.yaml"
        - name: MODELS_PATH
          value: "/app/models"
        - name: KB_PATH
          value: "/app/knowledge_base"
        - name: CACHE_ENABLED
          value: "true"
        - name: LOG_LEVEL
          value: "INFO"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
            ephemeral-storage: "5Gi"
          limits:
            memory: "4Gi"
            cpu: "2000m"
            ephemeral-storage: "10Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 180
          periodSeconds: 30
          timeoutSeconds: 15
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 5
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
        - name: shared-data
          mountPath: /app/models
          subPath: models
        - name: shared-data
          mountPath: /app/knowledge_base
          subPath: knowledge_base
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir:
          sizeLimit: 5Gi
      - name: shared-data
        emptyDir:
          sizeLimit: 50Gi
      nodeSelector:
        workload-type: gpu-inference
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        value: "present"
        effect: "NoSchedule"
      - key: "cloud.google.com/gke-preemptible"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
---
# Web Interface Service
apiVersion: v1
kind: Service
metadata:
  name: fm-llm-web-service
  namespace: fm-llm-prod
  labels:
    app: fm-llm-web
    component: web-interface
  annotations:
    cloud.google.com/neg: '{"ingress": true}'
    cloud.google.com/backend-config: '{"default": "fm-llm-backend-config"}'
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 5000
    protocol: TCP
    name: http
  selector:
    app: fm-llm-web
---
# Inference API Service
apiVersion: v1
kind: Service
metadata:
  name: fm-llm-inference-service
  namespace: fm-llm-prod
  labels:
    app: fm-llm-inference
    component: inference-api
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: fm-llm-inference
---
# Horizontal Pod Autoscaler for Web Interface
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fm-llm-web-hpa
  namespace: fm-llm-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fm-llm-web
  minReplicas: 2
  maxReplicas: 6
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 120
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60
---
# Horizontal Pod Autoscaler for Inference API
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fm-llm-inference-hpa
  namespace: fm-llm-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fm-llm-inference
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 85
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 180
    scaleDown:
      stabilizationWindowSeconds: 600
      policies:
      - type: Pods
        value: 1
        periodSeconds: 300
---
# Backend Configuration for Load Balancer
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
  name: fm-llm-backend-config
  namespace: fm-llm-prod
spec:
  timeoutSec: 120
  connectionDraining:
    drainingTimeoutSec: 60
  sessionAffinity:
    affinityType: "CLIENT_IP"
    affinityCookieTtlSec: 3600
  healthCheck:
    checkIntervalSec: 15
    timeoutSec: 10
    healthyThreshold: 1
    unhealthyThreshold: 3
    type: HTTP
    requestPath: /health
    port: 5000
---
# Ingress Configuration
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: fm-llm-ingress
  namespace: fm-llm-prod
  annotations:
    kubernetes.io/ingress.class: "gce"
    kubernetes.io/ingress.global-static-ip-name: "fm-llm-ip"
    networking.gke.io/managed-certificates: "fm-llm-ssl-cert"
    kubernetes.io/ingress.allow-http: "false"
    networking.gke.io/v1beta1.FrontendConfig: "fm-llm-frontend-config"
spec:
  rules:
  - host: fmgen.net
    http:
      paths:
      - path: /*
        pathType: ImplementationSpecific
        backend:
          service:
            name: fm-llm-web-service
            port:
              number: 80
  - host: www.fmgen.net
    http:
      paths:
      - path: /*
        pathType: ImplementationSpecific
        backend:
          service:
            name: fm-llm-web-service
            port:
              number: 80
---
# Frontend Configuration
apiVersion: networking.gke.io/v1beta1
kind: FrontendConfig
metadata:
  name: fm-llm-frontend-config
  namespace: fm-llm-prod
spec:
  redirectToHttps:
    enabled: true
    responseCodeName: MOVED_PERMANENTLY_DEFAULT
---
# Managed SSL Certificate
apiVersion: networking.gke.io/v1
kind: ManagedCertificate
metadata:
  name: fm-llm-ssl-cert
  namespace: fm-llm-prod
spec:
  domains:
  - fmgen.net
  - www.fmgen.net
---
# Network Policy for Security
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: fm-llm-network-policy
  namespace: fm-llm-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
  - from:
    - podSelector:
        matchLabels:
          app: fm-llm-web
  - from:
    - podSelector:
        matchLabels:
          app: fm-llm-inference
  egress:
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 80 