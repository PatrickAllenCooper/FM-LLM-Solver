steps:
# Build inference API image (will build CPU version and upgrade to GPU in deployment)
- name: 'gcr.io/cloud-builders/docker'
  args: 
  - 'build'
  - '-f'
  - 'Dockerfile.inference'
  - '-t'
  - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/fm-llm-inference:latest'
  - '.'

# Push inference API image
- name: 'gcr.io/cloud-builders/docker'
  args:
  - 'push'
  - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/fm-llm-inference:latest'

# Substitutions
substitutions:
  _LOCATION: 'us-central1'
  _REPOSITORY: 'fm-llm-repo'

# Options for larger build
options:
  substitution_option: 'ALLOW_LOOSE'
  machine_type: 'E2_HIGHCPU_32'
  disk_size_gb: '200'

# Extended timeout for ML dependencies
timeout: '2400s' 